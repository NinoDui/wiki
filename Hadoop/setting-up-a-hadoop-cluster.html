<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>Setting Up a Hadoop Cluster - Learning Notes of NinoDui</title>
    <meta name="keywords" content="Java, Python, MapReduce, Spark, AWS"/>
    <meta name="description" content="An ample-oriented collection of learning pieces."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#Hadoop">Hadoop</a>&nbsp;&#187;&nbsp;Setting Up a Hadoop Cluster
    <span class="updated">Page Updated&nbsp;
      2018-04-18 19:54
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">Setting Up a Hadoop Cluster</div>

  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#setting-up-a-hadoop-cluster">Setting up a Hadoop Cluster</a><ul>
<li><a href="#cluster-specification">Cluster Specification</a><ul>
<li><a href="#cluster-sizing-to-be-added">Cluster Sizing (to be added)</a></li>
<li><a href="#network-topology">Network Topology</a><ul>
<li><a href="#rack-awareness">Rack Awareness</a><ul>
<li><a href="#run-on-single-rack">Run on Single Rack</a></li>
<li><a href="#run-on-multrirack-clusters">Run on multrirack clusters</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#cluster-setup-and-installation">Cluster Setup and Installation</a><ul>
<li><a href="#installing-java-skip">Installing Java (Skip)</a></li>
<li><a href="#creating-unix-user-accounts">Creating Unix User Accounts</a></li>
<li><a href="#installing-hadoop-skip">Installing Hadoop (Skip)</a></li>
<li><a href="#configuring-ssh">Configuring SSH</a></li>
<li><a href="#configuring-hadooplater-in-detail">Configuring Hadoop(Later in Detail)</a></li>
<li><a href="#formatting-the-hdfs-filesystem">Formatting the HDFS FileSystem</a></li>
<li><a href="#starting-and-stopping-the-daemons">Starting and Stopping the Daemons</a></li>
<li><a href="#creating-user-directories">Creating User directories</a></li>
</ul>
</li>
<li><a href="#hadoop-configuration">Hadoop Configuration</a><ul>
<li><a href="#configuration-management">Configuration Management</a></li>
<li><a href="#environment-settings">Environment Settings</a><ul>
<li><a href="#java">Java</a></li>
<li><a href="#memory-heap-size">Memory heap size</a></li>
<li><a href="#system-logfiles">System logfiles</a></li>
<li><a href="#ssh-settings">SSH Settings</a></li>
</ul>
</li>
<li><a href="#important-hadoop-daemon-properties">Important Hadoop Daemon Properties</a><ul>
<li><a href="#hdfs">HDFS</a></li>
<li><a href="#yarn">YARN</a></li>
<li><a href="#memory-settings-in-yarn-and-mapreduce-to-be-added">Memory Settings in YARN and MapReduce (to be added)</a></li>
<li><a href="#cpu-settings-in-yarn-and-mapreduce-to-be-added">CPU Settings in YARN and MapReduce (to be added)</a></li>
</ul>
</li>
<li><a href="#hadoop-daemon-address-and-ports">Hadoop Daemon Address and Ports</a></li>
<li><a href="#other-hadoop-properties">Other Hadoop Properties</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h1 id="setting-up-a-hadoop-cluster">Setting up a Hadoop Cluster</h1>
<h2 id="cluster-specification">Cluster Specification</h2>
<h3 id="cluster-sizing-to-be-added">Cluster Sizing (to be added)</h3>
<h3 id="network-topology">Network Topology</h3>
<p>A common Hadoop cluster architecture consists of a two-level network topology (Rack + Switch)</p>
<h4 id="rack-awareness">Rack Awareness</h4>
<h5 id="run-on-single-rack">Run on Single Rack</h5>
<p>Nothing to do, default.</p>
<h5 id="run-on-multrirack-clusters">Run on multrirack clusters</h5>
<p>Network locations (nodes and racks)<br />
- represented in a tree<br />
- reflects the network “distance” between locations.<br />
- The Hadoop configuration <strong>must specify a map between node addresses and network locations</strong>.</p>
<div class="hlcode"><pre><span class="k">public</span> <span class="nf">interface</span> <span class="nx">DNSToSwitchMapping</span> <span class="p">{</span>
    <span class="k">public</span> <span class="nf">List</span><span class="o">&lt;</span><span class="kt">String</span><span class="o">&gt;</span> <span class="nx">resolve</span><span class="p">(</span><span class="nb">List</span><span class="o">&lt;</span><span class="kt">String</span><span class="o">&gt;</span> <span class="nx">names</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>The <code>net.topology.node.switch.mapping.impl</code> configuration property defines an implementation of the <code>DNSToSwitchMapping</code> interface that the namenode and the resource manager use to <strong>resolve worker node network locations</strong>.<br />
- Default implementation is <code>ScriptBasedMapping</code>, which runs a <strong>user-defined script</strong> to determine the mapping. Most installations don’t need to implement the interface themselves.<br />
- The script’s location is controlled by the property <code>net.topology.script.file.name</code>.<br />
- If <strong>no script location</strong> is specified, the default behavior is to <strong>map all nodes to a single network location</strong>, called <em>/default-rack</em>.</p>
<h2 id="cluster-setup-and-installation">Cluster Setup and Installation</h2>
<h3 id="installing-java-skip">Installing Java (Skip)</h3>
<h3 id="creating-unix-user-accounts">Creating Unix User Accounts</h3>
<p>The HDFS, MapReduce, and YARN services are usually run as separate users, named <code>hdfs</code>, <code>mapred</code>, and <code>yarn</code>, respectively. They all belong to the same <code>hadoop</code> group.</p>
<h3 id="installing-hadoop-skip">Installing Hadoop (Skip)</h3>
<h3 id="configuring-ssh">Configuring SSH</h3>
<p>Cluster-wide operations performed by:<br />
- Hadoop control scripts<br />
- Distributed Shell<br />
- Dedicated Hadoop management applications</p>
<div class="hlcode"><pre><span class="n">cat</span> <span class="o">~/</span><span class="p">.</span><span class="n">ssh</span><span class="o">/&lt;</span><span class="n">new</span><span class="o">-</span><span class="n">key</span><span class="o">&gt;</span><span class="p">.</span><span class="n">pub</span> <span class="o">&gt;&gt;</span> <span class="o">~/</span><span class="p">.</span><span class="n">ssh</span><span class="o">/</span><span class="n">authorized_keys</span>
</pre></div>


<h3 id="configuring-hadooplater-in-detail">Configuring Hadoop(Later in Detail)</h3>
<h3 id="formatting-the-hdfs-filesystem">Formatting the HDFS FileSystem</h3>
<p>Before it can be used, a brand-new HDFS installation needs to be formatted.<br />
- Creates an empty filesystem by creating the storage directories and the initial versions of the namenode’s persistent data structures.<br />
- <strong>Datanodes are not involved in the initial formatting process</strong>, since the namenode manages all of the filesystem’s metadata, and datanodes can join or leave the cluster dynamically.<br />
- <strong>Don’t need to say how large a filesystem to create</strong>, since this is determined by the number of datanodes in the cluster, which can be increased as needed, long after the filesystem is formatted.</p>
<div class="hlcode"><pre><span class="n">hdfs</span> <span class="n">namenode</span> <span class="o">-</span><span class="n">format</span>
</pre></div>


<h3 id="starting-and-stopping-the-daemons">Starting and Stopping the Daemons</h3>
<p>Hadoop comes with <strong>scripts</strong> (in <em>sbin</em>) across the whole cluster for<br />
- running commands<br />
- starting and stopping daemons</p>
<p>To use these scripts, we need to tell Hadoop which machines are in the cluster.<br />
- <code>slaves</code>, file for this purpose, which contains a list of the machine hostnames or IP addresses, one per line.<br />
- lists the machines that the datanodes and node managers should run on<br />
- resides in Hadoop’s configuration directory<br />
- may be replaced by <code>HADOOP_SLAVES</code> in <code>hadoop-env.sh</code><br />
- <strong>does not need to be distributed to worker nodes</strong>, since they are used only by the control scripts running on the namenode or resource manager.</p>
<div class="hlcode"><pre><span class="cp"># Start the HDFS daemon</span>
<span class="n">start</span><span class="o">-</span><span class="n">dfs</span><span class="p">.</span><span class="n">sh</span>

<span class="cp"># find the namenode&#39;s hostname from fs.defaultFS</span>
<span class="n">hdfs</span> <span class="n">getconf</span> <span class="o">-</span><span class="n">namenodes</span>

<span class="cp"># Start YARN</span>
<span class="n">start</span><span class="o">-</span><span class="n">yarn</span><span class="p">.</span><span class="n">sh</span>
</pre></div>


<p>These scripts start and stop Hadoop daemons using the <code>hadoop-daemon.sh</code> script (or the yarn-daemon.sh script, in the case of YARN).<br />
- If you use the aforementioned scripts, you <strong>shouldn’t</strong> call <code>hadoop-daemon.sh</code> directly.<br />
- If you need to control Hadoop daemons from another system or from your own scripts, use <code>hadoop-daemon.sh</code> script.</p>
<h3 id="creating-user-directories">Creating User directories</h3>
<div class="hlcode"><pre><span class="n">hadoop</span> <span class="n">fs</span> <span class="o">-</span><span class="n">mkdir</span> <span class="o">/</span><span class="n">user</span><span class="o">/</span><span class="n">username</span>
<span class="n">hadoop</span> <span class="n">fs</span> <span class="o">-</span><span class="n">chown</span> <span class="n">username</span><span class="o">:</span><span class="n">username</span> <span class="o">/</span><span class="n">suer</span><span class="o">/</span><span class="n">username</span>

<span class="cp"># Optional</span>
<span class="n">hdfs</span> <span class="n">dfsadmin</span> <span class="o">-</span><span class="n">setSpaceQuota</span> <span class="mi">1</span><span class="n">t</span> <span class="o">/</span><span class="n">user</span><span class="o">/</span><span class="n">username</span>
</pre></div>


<h2 id="hadoop-configuration">Hadoop Configuration</h2>
<ul>
<li><code>hadoop-env.sh</code> + <code>mapred-env.sh</code> + <code>yarn-env.sh</code><ul>
<li>bash script</li>
<li>Environment variables that are used in the scripts to run Hadoop/MapReduce/YARN</li>
<li>The last two <strong>overrides</strong> the former <code>hadoop-env.sh</code></li>
</ul>
</li>
<li><code>core-site.xml</code> <code>hdfs-site.xml</code> <code>mapred-site.xml</code> <code>yarn-site.xml</code><ul>
<li>Hadoop Configuration XML</li>
<li>Configuration settings for Hadoop Core/HDFS Daemons/MapReduce Daemons/YARN Daemons</li>
</ul>
</li>
<li><code>slaves</code><ul>
<li>Plain text</li>
<li>A list of machines (one per line) that each run a datanode and a node manager</li>
</ul>
</li>
<li><code>hadoop-metrics2.properties</code> <code>log4j.properties</code><ul>
<li>Java Properties</li>
</ul>
</li>
<li><code>hadoop-policy.xml</code><ul>
<li>Configuration settings for access control lists when running Hadoop in secure mode</li>
</ul>
</li>
</ul>
<p>These files are all found in the <code>etc/hadoop</code> directory of the Hadoop distribution.<br />
The configuration directory can be relocated to another part of the filesystems, as long as<br />
    - daemons are started with the --config option<br />
    - with the <code>HADOOP_CONF_DIR</code> environment variable set</p>
<h3 id="configuration-management">Configuration Management</h3>
<p>Hadoop does <strong>not</strong> have a single, global location for configuration information.<br />
Each Hadoop node in the cluster has its own set of configuration files, and it is up to administrators to ensure that they are kept in sync across the system. (Hadoop cluster management tools like <code>Cloudera Manager</code> and <code>Apache Ambari</code> take care of propagating changes across the cluster.)</p>
<p>If expand the cluster with new machines that have a different hardware specification from the existing ones, you need a different configuration, several excellent tools such as <code>Chef</code>, <code>Puppet</code>, <code>CFEngine</code>, and <code>Bcfg2</code>.</p>
<h3 id="environment-settings">Environment Settings</h3>
<p><code>mapred-env.sh</code> and <code>yarn-env.sh</code> files override the values set in <code>hadoop-env.sh</code>.</p>
<h4 id="java">Java</h4>
<ol>
<li><code>JAVA_HOME</code> in <code>hadoop-env.sh</code></li>
<li><code>JAVA_HOME</code> in shell environment variable (if not set in <code>hadoop-env.sh</code>)</li>
</ol>
<h4 id="memory-heap-size">Memory heap size</h4>
<ul>
<li>Default, 1000 MB (1GB) to each daemon</li>
<li>controlled by <code>HADOOP_HEAPSIZE</code> in <code>hadoop-env.sh</code></li>
</ul>
<h4 id="system-logfiles">System logfiles</h4>
<ul>
<li>Default <code>$HADOOP_HOME/logs</code></li>
<li>controlled by <code>HADOOP_LOG_DIR</code> in <code>hadoop-env.sh</code></li>
</ul>
<div class="hlcode"><pre><span class="n">export</span> <span class="n">HADOOP_LOG_DIR</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span>
</pre></div>


<p>Each Hadoop daemon running on a machine produces two logfiles.<br />
<em> The first<br />
    - written via log4j, name ends in <code>.log</code><br />
    - should be the first port of call when diagnosing problems because most application log messages are written here.<br />
    - The standard Hadoop log4j configuration uses a daily rolling file appender to rotate logfiles.<br />
    - Old logfiles are never deleted.<br />
</em> The second<br />
    - logfile is the combined standard output and standard error log. name ends in <code>.out</code><br />
    - rotated only when the daemon is restarted, and only the last five logs are retained.<br />
    - Old logfiles are suffixed with a number between 1 and 5, with 5 being the oldest file.</p>
<h4 id="ssh-settings">SSH Settings</h4>
<p>To pass extra options to SSH, define the <code>HADOOP_SSH_OPTS</code> environment variable in <code>hadoop-env.sh</code>.</p>
<h3 id="important-hadoop-daemon-properties">Important Hadoop Daemon Properties</h3>
<p>These properties are set in the Hadoop site files: <code>core-site.xml</code>, <code>hdfs-site.xml</code>, and <code>yarn-site.xml</code>.</p>
<p>To find the actual configuration of a running daemon, visit the <code>/conf</code> page on its web server. For example, http://resource-manager-host:8088/conf shows the configuration that the resource manager is running with.</p>
<h4 id="hdfs">HDFS</h4>
<p><code>fs.defaultFS</code><br />
<em> designate one machine as a namenode<br />
</em> an HDFS filesystem URI<br />
    - whose host is the namenode’s host‐name or IP address<br />
    - whose port is the port that the namenode will listen on for RPCs.<br />
* If no port is specified, the default of <strong>8020</strong> is used.</p>
<h4 id="yarn">YARN</h4>
<p>To run YARN, you need to <strong>designate one machine as a resource manager</strong>.<br />
* set the property <code>yarn.resourcemanager.hostname</code> to the hostname or IP address of the machine running the resource manager.</p>
<h4 id="memory-settings-in-yarn-and-mapreduce-to-be-added">Memory Settings in YARN and MapReduce (to be added)</h4>
<h4 id="cpu-settings-in-yarn-and-mapreduce-to-be-added">CPU Settings in YARN and MapReduce (to be added)</h4>
<h3 id="hadoop-daemon-address-and-ports">Hadoop Daemon Address and Ports</h3>
<h3 id="other-hadoop-properties">Other Hadoop Properties</h3>
  <div class="relation">
    <h2>Related</h2>
    <ul>
    
    <li><a href="/wiki/Hadoop/mapreduce.html">MapReduce</a></li>
    
    </ul>
  </div>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2018 NinoDui.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2018-04-20 12:01:59</p>
      </span>
    </div>

    
    
  </body>
</html>